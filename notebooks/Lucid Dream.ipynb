{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ryan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import gpt_2_simple as gpt2\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "import contextlib\n",
    "import re\n",
    "\n",
    "sys.path.append(\"../lib/InferSent\")\n",
    "from models import InferSent\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18481(/19809) words with w2v vectors\n",
      "Vocab size : 18481\n"
     ]
    }
   ],
   "source": [
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048, 'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load('../models/encoder/infersent2.pkl'))\n",
    "infersent.set_w2v_path(\"../models/fastText/crawl-300d-2M.vec\")\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "squad_df = pd.read_csv(\"../corpora/squad-dev-v2.0.csv\", index_col=0)\n",
    "\n",
    "sentences = []\n",
    "\n",
    "contexts = list(squad_df[\"contexts\"].drop_duplicates())\n",
    "for context in contexts:\n",
    "    doc = nlp(context)\n",
    "    sentences += [sentence.string.strip() for sentence in doc.sents]\n",
    "    \n",
    "infersent.build_vocab(sentences, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(text1, text2):\n",
    "    return np.dot(text1, text2)/(np.linalg.norm(text1) * np.linalg.norm(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    return [sentence.string.strip() for sentence in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_predict(statements, question):\n",
    "    context_sentences = split_sentences(statements)\n",
    "    most_similar, highest_sim = \"\", 0\n",
    "    for sentence in context_sentences:\n",
    "        similarity = cosine_similarity(infersent.encode([question])[0], infersent.encode([sentence])[0])\n",
    "        if similarity > highest_sim:\n",
    "            most_similar = sentence\n",
    "            highest_sim = similarity\n",
    "    return most_similar, highest_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = {}\n",
    "profile_dir = \"../corpora/profiles\"\n",
    "for profile in os.listdir(profile_dir):\n",
    "    df = pd.read_csv(f\"{profile_dir}/{profile}\")\n",
    "    character = df.columns.tolist()[0]\n",
    "    profiles[character] = {}\n",
    "    for emotion in df.columns.tolist()[1:]:\n",
    "        profiles[character][emotion] = df[emotion].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotional_composite(emotional_profile, response_length):\n",
    "    composite_amount = random.randint(1,3)\n",
    "    emotions, probabilities = zip(*emotional_profile.items())\n",
    "    response_breakdown = [(str(emotion), math.floor(response_length/composite_amount) + int(i < response_length % composite_amount)) for i, emotion in enumerate(np.random.choice(emotions, composite_amount, p=probabilities))]\n",
    "    return response_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_composite_response(sess, emotional_profile, conversation, character, response_length=30):\n",
    "    response = \"\\n\".join([f\"{sentence[0]}: {sentence[1]}\" for sentence in conversation]) + f\"\\n{character}:\"\n",
    "    start_offset = len(response)\n",
    "    response_breakdown = get_emotional_composite(emotional_profile, response_length)\n",
    "    for emotion, length in response_breakdown:\n",
    "        gpt2.reset_session(sess)\n",
    "        sess = gpt2.start_tf_sess()\n",
    "        gpt2.load_gpt2(sess, run_name=f\"{emotion}_run1\")\n",
    "        response = gpt2.generate(\n",
    "            sess,\n",
    "            length=length,\n",
    "            temperature=0.7,\n",
    "            prefix=response,\n",
    "            nsamples=1,\n",
    "            batch_size=1,\n",
    "            run_name=f\"{emotion}_run1\",\n",
    "            return_as_list=True\n",
    "        )[0]\n",
    "    return re.split(r\"[a-z A-Z0-9]+:\", response[start_offset:])[0].strip().split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_character_response(sess, profile, conversation, character, response_length=30):\n",
    "    seed = \"\\n\".join([f\"{sentence[0]}: {sentence[1]}\" for sentence in conversation]) + f\"\\n{character}:\"\n",
    "    gpt2.reset_session(sess)\n",
    "    sess = gpt2.start_tf_sess()\n",
    "    gpt2.load_gpt2(sess, run_name=f\"{character}_run1\")\n",
    "    response = gpt2.generate(\n",
    "        sess,\n",
    "        length=response_length,\n",
    "        temperature=0.7,\n",
    "        prefix=seed,\n",
    "        nsamples=1,\n",
    "        batch_size=1,\n",
    "        run_name=f\"{character}_run1\",\n",
    "        return_as_list=True\n",
    "    )[0][len(seed):]\n",
    "    return re.split(r\"[a-z A-Z0-9]+:\", response)[0].strip().split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holistic_model_response(sess, conversation, character, response_length=25):\n",
    "    seed = \"\\n\".join([f\"{sentence[0]}: {sentence[1]}\" for sentence in conversation]) + f\"\\n{character}:\"\n",
    "    gpt2.reset_session(sess)\n",
    "    sess = gpt2.start_tf_sess()\n",
    "    gpt2.load_gpt2(sess, run_name=f\"full_model_run1\")\n",
    "    response = gpt2.generate(\n",
    "        sess,\n",
    "        length=response_length,\n",
    "        temperature=0.7,\n",
    "        prefix=seed,\n",
    "        nsamples=1,\n",
    "        batch_size=1,\n",
    "        run_name=f\"{character}_run1\",\n",
    "        return_as_list=True\n",
    "    )[0][len(seed):]\n",
    "    return re.split(r\"[a-z A-Z0-9]+:\", response)[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_conversation(\n",
    "    conversation=[], \n",
    "    scene=[\"harry\", \"user\", \"environment\"], \n",
    "    characters=[\"harry\", \"ron\", \"hermione\", \"snape\", \"albus dumbledore\", \"tom riddle\", \"hagrid\", \"user\", \"environment\"],\n",
    "    character_addition_prob=0.2,\n",
    "    character_removal_prob=0.25,\n",
    "    env_model=generate_character_response, \n",
    "    char_model=generate_composite_response, \n",
    "    length=10,\n",
    "    print_scene=False\n",
    "):\n",
    "    [print(f\"{character}: {statement}\") for character, statement in conversation]\n",
    "    sess = gpt2.start_tf_sess()\n",
    "    for i in range(length):\n",
    "        if i and random.random() < character_removal_prob:\n",
    "            del scene[scene.index(random.choice(scene))]\n",
    "        if i and random.random() < character_addition_prob:\n",
    "            scene.append(random.choice([character for character in characters if character not in scene]))\n",
    "        if not scene:\n",
    "            break\n",
    "        with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "            character = random.choice([character for character in scene if not conversation or character != conversation[-1][0]])\n",
    "            if character == \"user\":\n",
    "                print(\"user: \", end=\"\")\n",
    "                response = input(\"user: \")\n",
    "            elif character == \"environment\":\n",
    "                response = env_model(sess, profiles[character], conversation, \"environment\")\n",
    "            else:\n",
    "                response = char_model(sess, profiles[character], conversation, character)\n",
    "        if character != \"user\":\n",
    "            print(f\"{character.capitalize()}: {response}{' - ' + str(scene) if print_scene else ''}\")\n",
    "        conversation.append((character, response))\n",
    "        if len(conversation) > 5:\n",
    "            predicted_response, _ = cosine_predict(\"\\n\".join([statement for _, statement in conversation[:-1]]), response)\n",
    "            conversation.append((character, predicted_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_setup = [\n",
    "    (\"environment\", \"Diagon Alley was completely empty because of the coronavirus.\"),\n",
    "    (\"hagrid\", \"Where is everybody?\"),\n",
    "    (\"albus dumbledore\", \"Hopefully at home.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment: Diagon Alley was completely empty because of the coronavirus.\n",
      "hagrid: Where is everybody?\n",
      "albus dumbledore: Hopefully at home.\n",
      "Hermione: We still don't know for sure what it is doing, but it's causing widespread illness and deaths. We'll have to be extra careful. - ['harry', 'ron', 'hermione']\n",
      "Ron: You don't think St. Nicholas will turn out to be the monster, do you? - ['harry', 'ron']\n",
      "Harry: I think if he did, he'd be dead. - ['harry', 'ron']\n",
      "Tom riddle: I knew it wouldn't be safe to open the Chamber again while I was still at school. So I decided to leave behind a diary, preserving my - ['harry', 'tom riddle']\n",
      "Harry: You don't think St. Nicholas will turn out to be the monster, do you? - ['harry']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Cannot choose from an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-dba39cb7df5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mchar_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_character_response\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint_scene\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-13-e1cb044c25e9>\u001b[0m in \u001b[0;36mstart_conversation\u001b[0;34m(conversation, scene, characters, character_addition_prob, character_removal_prob, env_model, char_model, length, print_scene)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mscene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcharacter\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcharacters\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevnull\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mredirect_stdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mcharacter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcharacter\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscene\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconversation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot choose from an empty sequence'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Cannot choose from an empty sequence"
     ]
    }
   ],
   "source": [
    "start_conversation(\n",
    "    conversation=scene_setup, \n",
    "    scene=[\"harry\", \"ron\", \"hermione\"], \n",
    "    characters=[\"harry\", \"ron\", \"hermione\", \"snape\", \"albus dumbledore\", \"tom riddle\", \"hagrid\", \"user\", \"environment\"],\n",
    "    character_addition_prob=0.2,\n",
    "    character_removal_prob=0.25,\n",
    "    env_model=generate_character_response, \n",
    "    char_model=generate_character_response, \n",
    "    length=10,\n",
    "    print_scene=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
