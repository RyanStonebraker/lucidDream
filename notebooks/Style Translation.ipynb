{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.7-W2, 0.8-W3, 0.9-W4, WORD_1, 0.9-W5, 0.8-W6, 0.7-W7\n",
    "# 0.7-W2, 0.8-W3, 0.9-W4, WORD_2, 0.9-W5, 0.8-W6, 0.7-W7\n",
    "# WORD_1 == WORD_2, translate WORD_1 -> WORD_2 based on CBOW embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {}\n",
    "emotions[\"neutral\"] = \"I just got a new car.\"\n",
    "emotions[\"joy\"] = \"I love my awesome new car!\"\n",
    "emotions[\"disgust\"] = \"I just got a crappy new car.\"\n",
    "emotions[\"guilt\"] = \"I spent all my money on an expensive new car.\"\n",
    "emotions[\"fear\"] = \"I hope I do not crash my expensive new car.\"\n",
    "emotions[\"anger\"] = \"I hate my bad new car.\"\n",
    "emotions[\"shame\"] = \"I am embarassed by my clunky new car.\"\n",
    "emotions[\"sadness\"] = \"I wish I could have gotten a better new car.\"\n",
    "# awesome -> crappy -> expensive -> clunky -> better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neutral': [6, 20, 5, 32, 7, 18, 29],\n",
       " 'joy': [6, 19, 25, 21, 7, 18, 27],\n",
       " 'disgust': [6, 20, 5, 32, 1, 7, 18, 29],\n",
       " 'guilt': [6, 28, 12, 25, 31, 13, 4, 24, 7, 18, 29],\n",
       " 'fear': [6, 14, 6, 15, 17, 26, 25, 24, 7, 18, 29],\n",
       " 'anger': [6, 23, 25, 8, 7, 18, 29],\n",
       " 'shame': [6, 9, 22, 11, 25, 0, 7, 18, 29],\n",
       " 'sadness': [6, 16, 6, 3, 10, 2, 32, 30, 7, 18, 29]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = {}\n",
    "tokenized_sentences = {}\n",
    "language = set()\n",
    "for emotion, sentence in emotions.items():\n",
    "    tokenized_sentence = nltk.tokenize.word_tokenize(sentence)\n",
    "    language.update(tokenized_sentence)\n",
    "    tokenized_sentences[emotion] = tokenized_sentence\n",
    "\n",
    "embedding = {word: i for i, word in enumerate(language)}\n",
    "embedded_sentences = {emotion: [embedding[word] for word in sentence] for emotion, sentence in tokenized_sentences.items()}\n",
    "embedded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_occurences = {}\n",
    "for embedded in [embedding for _, embedding in embedded_sentences.items()]:\n",
    "    for word in embedded:\n",
    "        word_occurences[word] = word_occurences[word] + 1 if word in word_occurences else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_freq():\n",
    "    return {embedding[word]: 0 for word in language}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_window = {}\n",
    "window_size = 3\n",
    "for embedded in [embedding for _, embedding in embedded_sentences.items()]:\n",
    "    for main_i, main_word in enumerate(embedded):\n",
    "        context_window[main_word] = context_window[main_word] if main_word in context_window else {\"pre\": {}, \"post\": {}}\n",
    "        for compare_i, compare_word in enumerate(embedded[main_i - window_size if main_i > window_size else 0:main_i + window_size + 1]):\n",
    "            if main_word == compare_word:\n",
    "                continue\n",
    "            placement = \"pre\" if compare_i < main_i else \"post\"\n",
    "            distance = abs(window_size - compare_i)\n",
    "            if distance not in context_window[main_word][placement]:\n",
    "                context_window[main_word][placement][distance] = get_language_freq()\n",
    "            context_window[main_word][placement][distance][compare_word] += float(1/word_occurences[compare_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pre': {3: {0: 0,\n",
       "   1: 0,\n",
       "   2: 0,\n",
       "   3: 0,\n",
       "   4: 0,\n",
       "   5: 0,\n",
       "   6: 0.1,\n",
       "   7: 0,\n",
       "   8: 0,\n",
       "   9: 0,\n",
       "   10: 0,\n",
       "   11: 0,\n",
       "   12: 0,\n",
       "   13: 0,\n",
       "   14: 0,\n",
       "   15: 0,\n",
       "   16: 0,\n",
       "   17: 0,\n",
       "   18: 0,\n",
       "   19: 0,\n",
       "   20: 0,\n",
       "   21: 0,\n",
       "   22: 0,\n",
       "   23: 0,\n",
       "   24: 0,\n",
       "   25: 0,\n",
       "   26: 0,\n",
       "   27: 0,\n",
       "   28: 0,\n",
       "   29: 0,\n",
       "   30: 0,\n",
       "   31: 0,\n",
       "   32: 0},\n",
       "  2: {0: 0,\n",
       "   1: 0,\n",
       "   2: 0,\n",
       "   3: 0,\n",
       "   4: 0,\n",
       "   5: 0,\n",
       "   6: 0,\n",
       "   7: 0,\n",
       "   8: 0,\n",
       "   9: 0,\n",
       "   10: 0,\n",
       "   11: 0,\n",
       "   12: 0,\n",
       "   13: 0,\n",
       "   14: 0,\n",
       "   15: 0,\n",
       "   16: 0,\n",
       "   17: 0,\n",
       "   18: 0,\n",
       "   19: 1.0,\n",
       "   20: 0,\n",
       "   21: 0,\n",
       "   22: 0,\n",
       "   23: 0,\n",
       "   24: 0,\n",
       "   25: 0,\n",
       "   26: 0,\n",
       "   27: 0,\n",
       "   28: 0,\n",
       "   29: 0,\n",
       "   30: 0,\n",
       "   31: 0,\n",
       "   32: 0},\n",
       "  1: {0: 0,\n",
       "   1: 0,\n",
       "   2: 0,\n",
       "   3: 0,\n",
       "   4: 0,\n",
       "   5: 0,\n",
       "   6: 0,\n",
       "   7: 0,\n",
       "   8: 0,\n",
       "   9: 0,\n",
       "   10: 0,\n",
       "   11: 0,\n",
       "   12: 0,\n",
       "   13: 0,\n",
       "   14: 0,\n",
       "   15: 0,\n",
       "   16: 0,\n",
       "   17: 0,\n",
       "   18: 0,\n",
       "   19: 0,\n",
       "   20: 0,\n",
       "   21: 0,\n",
       "   22: 0,\n",
       "   23: 0,\n",
       "   24: 0,\n",
       "   25: 0.2,\n",
       "   26: 0,\n",
       "   27: 0,\n",
       "   28: 0,\n",
       "   29: 0,\n",
       "   30: 0,\n",
       "   31: 0,\n",
       "   32: 0}},\n",
       " 'post': {1: {0: 0,\n",
       "   1: 0,\n",
       "   2: 0,\n",
       "   3: 0,\n",
       "   4: 0,\n",
       "   5: 0,\n",
       "   6: 0,\n",
       "   7: 0.125,\n",
       "   8: 0,\n",
       "   9: 0,\n",
       "   10: 0,\n",
       "   11: 0,\n",
       "   12: 0,\n",
       "   13: 0,\n",
       "   14: 0,\n",
       "   15: 0,\n",
       "   16: 0,\n",
       "   17: 0,\n",
       "   18: 0,\n",
       "   19: 0,\n",
       "   20: 0,\n",
       "   21: 0,\n",
       "   22: 0,\n",
       "   23: 0,\n",
       "   24: 0,\n",
       "   25: 0,\n",
       "   26: 0,\n",
       "   27: 0,\n",
       "   28: 0,\n",
       "   29: 0,\n",
       "   30: 0,\n",
       "   31: 0,\n",
       "   32: 0},\n",
       "  2: {0: 0,\n",
       "   1: 0,\n",
       "   2: 0,\n",
       "   3: 0,\n",
       "   4: 0,\n",
       "   5: 0,\n",
       "   6: 0,\n",
       "   7: 0,\n",
       "   8: 0,\n",
       "   9: 0,\n",
       "   10: 0,\n",
       "   11: 0,\n",
       "   12: 0,\n",
       "   13: 0,\n",
       "   14: 0,\n",
       "   15: 0,\n",
       "   16: 0,\n",
       "   17: 0,\n",
       "   18: 0.125,\n",
       "   19: 0,\n",
       "   20: 0,\n",
       "   21: 0,\n",
       "   22: 0,\n",
       "   23: 0,\n",
       "   24: 0,\n",
       "   25: 0,\n",
       "   26: 0,\n",
       "   27: 0,\n",
       "   28: 0,\n",
       "   29: 0,\n",
       "   30: 0,\n",
       "   31: 0,\n",
       "   32: 0},\n",
       "  3: {0: 0,\n",
       "   1: 0,\n",
       "   2: 0,\n",
       "   3: 0,\n",
       "   4: 0,\n",
       "   5: 0,\n",
       "   6: 0,\n",
       "   7: 0,\n",
       "   8: 0,\n",
       "   9: 0,\n",
       "   10: 0,\n",
       "   11: 0,\n",
       "   12: 0,\n",
       "   13: 0,\n",
       "   14: 0,\n",
       "   15: 0,\n",
       "   16: 0,\n",
       "   17: 0,\n",
       "   18: 0,\n",
       "   19: 0,\n",
       "   20: 0,\n",
       "   21: 0,\n",
       "   22: 0,\n",
       "   23: 0,\n",
       "   24: 0,\n",
       "   25: 0,\n",
       "   26: 0,\n",
       "   27: 1.0,\n",
       "   28: 0,\n",
       "   29: 0,\n",
       "   30: 0,\n",
       "   31: 0,\n",
       "   32: 0}}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_window[embedding[\"awesome\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_contexts(context_window, embedding):\n",
    "    context = []\n",
    "    for placement_name, placement in context_window[embedding].items():\n",
    "        for _, ordering in placement.items():\n",
    "            \n",
    "            context.append([freq for freq in sorted(ordering.items(), key=operator.itemgetter(1)) if freq[1] > 0])\n",
    "        if placement_name == \"pre\":\n",
    "            context.append([(embedding, 1.0)])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_embedding = {val:key for key,val in embedding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_contexts_to_words(contexts, complete_length, start=\"\"):\n",
    "    sentences = []\n",
    "    if not contexts and len(start.split(\" \")) == complete_length:\n",
    "        return [start]\n",
    "    for i, neighbor in enumerate(contexts):\n",
    "        for j, possible_neighbor in enumerate(neighbor):\n",
    "            sentences += convert_contexts_to_words(contexts[i+1:], complete_length, f\"{start+' ' if start else start}{reverse_embedding[possible_neighbor[0]]}\")\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_context_to_sentences(contexts):\n",
    "    return convert_contexts_to_words(contexts, len(contexts), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(29, 0.2857142857142857), (17, 1.0), (31, 1.0)],\n",
       " [(18, 0.25), (13, 1.0), (26, 1.0)],\n",
       " [(25, 0.2), (7, 0.25), (4, 1.0)],\n",
       " [(24, 1.0)]]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_contexts(context_window, embedding[\"expensive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[\".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre\n",
      "post\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['. car my expensive',\n",
       " '. car new expensive',\n",
       " '. car an expensive',\n",
       " '. on my expensive',\n",
       " '. on new expensive',\n",
       " '. on an expensive',\n",
       " '. crash my expensive',\n",
       " '. crash new expensive',\n",
       " '. crash an expensive',\n",
       " 'not car my expensive',\n",
       " 'not car new expensive',\n",
       " 'not car an expensive',\n",
       " 'not on my expensive',\n",
       " 'not on new expensive',\n",
       " 'not on an expensive',\n",
       " 'not crash my expensive',\n",
       " 'not crash new expensive',\n",
       " 'not crash an expensive',\n",
       " 'money car my expensive',\n",
       " 'money car new expensive',\n",
       " 'money car an expensive',\n",
       " 'money on my expensive',\n",
       " 'money on new expensive',\n",
       " 'money on an expensive',\n",
       " 'money crash my expensive',\n",
       " 'money crash new expensive',\n",
       " 'money crash an expensive']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_context_to_sentences(show_contexts(context_window, embedding[\"expensive\"]), )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
